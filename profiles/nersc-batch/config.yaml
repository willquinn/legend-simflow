# profit from Perlmutter's scratch area: https://docs.nersc.gov/filesystems/perlmutter-scratch
# see: https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-can-i-make-use-of-node-local-storage-when-running-cluster-jobs
# NOTE: this only works if paths to input/output files are NOT absolute! you
# must not use the $_ special variable in config.json in front of paths that
# should live in the scratch area and just specify a relative path.
shadow-prefix: $PSCRATCH

# NERSC uses the SLURM job scheduler
# - https://snakemake.readthedocs.io/en/stable/executing/cluster.html#executing-on-slurm-clusters
slurm: true

# maximum number of cores used locally, on the interactive node
local-cores: 100
# maximum number of jobs that can exist in the SLURM queue at a time
jobs: 100

# set resource limits to single job. at NERSC, the total wall time must be less
# than 12h and a node offers max 512GB memory
resources:
  - runtime=720
  - mem_mb=512000

# reasonable defaults that do not stress the scheduler
max-jobs-per-second: 20
max-status-checks-per-second: 20

# (LEGEND) NERSC-specific settings
# - https://snakemake.readthedocs.io/en/stable/executing/cluster.html#advanced-resource-specifications
# - https://docs.nersc.gov/jobs
default-resources:
  - slurm_account="m2676"
  - constraint="cpu"
  - runtime=120
  - mem_mb=1000
  - slurm_extra="--qos regular --licenses scratch,cfs" # --mail-type end,fail --mail-user luigi.pertoldi@tum.d

# number of threads used by each rule
set-threads:
  - build_tier_ver=1
  - build_tier_raw=1

# we define groups in order to let Snakemake group rule instances in the same
# SLURM job. relevant docs:
# - https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#snakefiles-grouping
# - https://snakemake.readthedocs.io/en/stable/executing/grouping.html#job-grouping
groups:
  - build_tier_ver=sims
  - build_tier_raw=sims

# disconnected parts of the workflow can run in parallel (at most 256 of them)
# in a group
group-components:
    - sims=256
